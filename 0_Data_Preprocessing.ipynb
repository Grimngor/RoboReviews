{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Pre-Processing\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing of the Amazon Reviews dataset \"\"\"\n",
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd \n",
    "import polars as pl # pylint: disable=E0401\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset # pylint: disable=E0401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Datasets and saving them locally\n",
    "This section is from older versions of the code. Once I had the originals I started using the next version.\n",
    "In those older versions there is code for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control variables for code execution\n",
    "DOWNLOAD_ORIGINAL_DATASETS = False\n",
    "SAVE_LOCAL_DATASETS = False\n",
    "\n",
    "if DOWNLOAD_ORIGINAL_DATASETS:\n",
    "    # Get all available configurations (sub-categories)\n",
    "    categories = [\n",
    "        \"raw_review_All_Beauty\",\n",
    "        \"raw_review_Electronics\",\n",
    "        \"raw_review_Office_Products\",\n",
    "        \"raw_meta_All_Beauty\",\n",
    "        \"raw_meta_Electronics\",\n",
    "        \"raw_meta_Office_Products\",\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty list to hold datasets\n",
    "    datasets_list = []\n",
    "\n",
    "    # Loop over each configuration and download the dataset\n",
    "    for category in categories:\n",
    "        dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", category)\n",
    "        datasets_list.append(dataset)\n",
    "\n",
    "if SAVE_LOCAL_DATASETS:\n",
    "    # Save each dataset locally\n",
    "    for ds, category in zip(datasets_list, categories):\n",
    "        ds.save_to_disk(f\"./{category}_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all individual Dataframes from local CSVs\n",
    "Executed in parallel to reduce load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel loading of pre-saved dataframes\n",
    "file_paths = [\n",
    "    \"./dataframes/small_df.csv\",\n",
    "    \"./dataframes/beauty_df.csv\",\n",
    "    \"./dataframes/electronics_df.csv\",\n",
    "    \"./dataframes/office_df.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_csv(path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a csv file into a polars dataframe\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the csv file\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame\n",
    "    \"\"\"\n",
    "    return pl.read_csv(path)\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dataframes = list(executor.map(load_csv, file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "### Defining Functions for Removing short Reviews, Cleaning nulls and Duplicates ans Slicing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmv_short_long(\n",
    "    dataframe: pl.DataFrame, min_word_count: int = 25, max_word_count: int = 512\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows from a polars dataframe where the text of the 'text' column\n",
    "    is shorter than 'min_word_count' or longer than 'max_word_count'\n",
    "\n",
    "    Args:\n",
    "        dataframe (pl.DataFrame)\n",
    "        min_word_count (int)\n",
    "        max_word_count (int)\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame\n",
    "    \"\"\"\n",
    "    # Filter the dataframe\n",
    "    word_count_filtered_df = dataframe.filter(\n",
    "        (pl.col(\"text\").str.split(\" \").list.len() >= min_word_count)\n",
    "        & (pl.col(\"text\").str.split(\" \").list.len() <= max_word_count)\n",
    "    )\n",
    "    \n",
    "    # Calculate number and percentage of rows removed\n",
    "    rows_removed = dataframe.height - word_count_filtered_df.height\n",
    "    percentage_removed = (rows_removed / dataframe.height) * 100\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Rows removed: {rows_removed} ({percentage_removed:.2f}% of the total)\")\n",
    "\n",
    "    return word_count_filtered_df\n",
    "\n",
    "# Remove Duplicates and Nulls.\n",
    "# Lazy is used for more efficient execution of the multiple operations in the dataframes\n",
    "def clean_data(dataframe: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicates, None values, and rows where 'rating' is 0 from a Polars dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pl.DataFrame)\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame\n",
    "    \"\"\"\n",
    "    df_cleaned = (\n",
    "        dataframe.lazy()\n",
    "        .unique(subset=[\"text\"])  # Remove duplicates based on 'text' column\n",
    "        .drop_nulls()  # Drop rows with None values\n",
    "        .filter(pl.col(\"rating\") != 0)  # Filter out rows where 'rating' is 0\n",
    "    )\n",
    "    return df_cleaned.collect()\n",
    "\n",
    "# Split into n parts using slicing\n",
    "# Used in previous version sfor processing large datasets\n",
    "def slice_pl_df(dataframe: pl.DataFrame, slices: int) -> list[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Slices a polars dataframes into 'slices' parts\n",
    "\n",
    "    Args:\n",
    "        dataframe (pl.DataFrame)\n",
    "        slices (int): Number of slices or parts\n",
    "    Returns:\n",
    "        list[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    slice_size = dataframe.shape[0] // slices\n",
    "    dfs = []\n",
    "\n",
    "    for df_slice in tqdm(range(slices)):\n",
    "        start_idx = df_slice * slice_size\n",
    "        if df_slice == slices - 1:  # Ensure the last slice takes all remaining rows\n",
    "            dfs.append(dataframe[start_idx:])\n",
    "        else:\n",
    "            end_idx = (df_slice + 1) * slice_size\n",
    "            dfs.append(dataframe[start_idx:end_idx])\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the final dataset\n",
    "The individual datasets are processed and then combined into one by sampling with an equal distribution of review ratings and individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling and grouping dataframes: 100%|██████████| 4/4 [00:00<00:00,  9.23it/s]\n",
      "Cleaning dataframes:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 5 (0.07% of the total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning dataframes:  50%|█████     | 2/4 [00:00<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 505 (0.24% of the total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning dataframes:  75%|███████▌  | 3/4 [00:19<00:08,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 55932 (1.86% of the total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning dataframes: 100%|██████████| 4/4 [00:29<00:00,  7.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed: 11314 (0.38% of the total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling groups: 100%|██████████| 3/3 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame shape: (187494, 8)\n",
      "\n",
      "Group distribution:\n",
      "shape: (3, 2)\n",
      "┌──────────┬───────┐\n",
      "│ group    ┆ len   │\n",
      "│ ---      ┆ ---   │\n",
      "│ str      ┆ u32   │\n",
      "╞══════════╪═══════╡\n",
      "│ Neutral  ┆ 62498 │\n",
      "│ Negative ┆ 62498 │\n",
      "│ Positive ┆ 62498 │\n",
      "└──────────┴───────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "FINAL_DF_ROWS = 250_000\n",
    "LARGE_SAMPLE_SIZE = 1_000_000\n",
    "\n",
    "selected_columns = [\n",
    "    \"title\",\n",
    "    \"text\",\n",
    "    \"rating\",\n",
    "    \"id\",\n",
    "    \"parent_asin\",\n",
    "    \"name\",\n",
    "    \"categories\",\n",
    "]\n",
    "groups = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "rating_to_group = {\n",
    "    1.0: \"Negative\",\n",
    "    2.0: \"Negative\",\n",
    "    3.0: \"Neutral\",\n",
    "    4.0: \"Positive\",\n",
    "    5.0: \"Positive\",\n",
    "}\n",
    "\n",
    "# Drop unwanted columns and null values\n",
    "dataframes = [df[selected_columns] for df in dataframes]\n",
    "\n",
    "small_dataframes = []\n",
    "for df in tqdm(dataframes, desc=\"Sampling and grouping dataframes\"):\n",
    "    # Sample the dataframe if its size exceeds LARGE_SAMPLE_SIZE\n",
    "    if df.height > LARGE_SAMPLE_SIZE:\n",
    "        df = df.sample(LARGE_SAMPLE_SIZE)\n",
    "    \n",
    "    # Add a new 'group' column by mapping the 'rating' column\n",
    "    df = df.with_columns(\n",
    "        group=pl.col(\"rating\").replace_strict(rating_to_group)\n",
    "    )\n",
    "    \n",
    "    small_dataframes.append(df)\n",
    "\n",
    "# Slice large dataframes into smaller chunks to improve memory efficiency\n",
    "# Not used in this version because we sample instead.\n",
    "\"\"\" for df in tqdm(dataframes, desc=\"Slicing dataframes\"):\n",
    "    if df.height > LARGE_SAMPLE_SIZE:\n",
    "        df = slice_pl_df(df, 10) \"\"\"\n",
    "\n",
    "# Clean and preprocess the dataframes.\n",
    "for df in tqdm(small_dataframes, desc=\"Cleaning dataframes\"):\n",
    "    if isinstance(df, list):\n",
    "        cleaned_dfs = []\n",
    "        for sub_df in df:\n",
    "            sub_df = clean_data(sub_df)\n",
    "            sub_df = rmv_short_long(sub_df)\n",
    "            cleaned_dfs.append(sub_df)\n",
    "        df = pl.concat(cleaned_dfs)\n",
    "    else:\n",
    "        df = clean_data(df)\n",
    "        df = rmv_short_long(df)\n",
    "\n",
    "# Determine total required rows per group\n",
    "all_group_counts = (\n",
    "    pl.concat([df.select(\"group\") for df in small_dataframes])\n",
    "    .group_by(\"group\")\n",
    "    .len()\n",
    ")\n",
    "\n",
    "rows_per_group = FINAL_DF_ROWS // all_group_counts.height\n",
    "\n",
    "# Initialize a list to collect samples\n",
    "final_samples = []\n",
    "\n",
    "# Allocate and sample data for each group\n",
    "for group in tqdm(groups, desc=\"Sampling groups\"):\n",
    "    # Calculate the total available rows for the group across all datasets\n",
    "    total_available = sum(\n",
    "        df.filter(pl.col(\"group\") == group).height for df in small_dataframes\n",
    "    )\n",
    "    required_rows_adjusted = min(rows_per_group, total_available)\n",
    "\n",
    "    # Ideally, each dataset contributes equally\n",
    "    per_dataset_required = required_rows_adjusted // len(small_dataframes)\n",
    "\n",
    "    # Sample data from each dataset for the group\n",
    "    for df in small_dataframes:\n",
    "        df_group = df.filter(pl.col(\"group\") == group)\n",
    "        available_rows = df_group.height\n",
    "        sample_size = min(per_dataset_required, available_rows)\n",
    "        if sample_size > 0:\n",
    "            sample_df = df_group.sample(n=sample_size)\n",
    "            final_samples.append(sample_df)\n",
    "\n",
    "# Concatenate all sampled data into the final DataFrame\n",
    "final_df = pl.concat(final_samples)\n",
    "\n",
    "# Verify the final DataFrame\n",
    "print(f\"\\nFinal DataFrame shape: {final_df.shape}\")\n",
    "print(\"\\nGroup distribution:\")\n",
    "print(final_df.group_by(\"group\").len())\n",
    "\n",
    "final_df = final_df.drop(\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV atomically\n",
    "final_df.write_csv(\"final_dataset_200k.csv.tmp\")\n",
    "os.replace(\"final_dataset_200k.csv.tmp\", \"final_dataset_200k.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoboReviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
