{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM, PretrainedConfig, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "final_df = pd.read_pickle('final_dataset_clust.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd7a4436d6a4b7a9012d93d3fe25190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "login(\"your key\")\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Download the config.json file and load it\n",
    "config_path = hf_hub_download(repo_id=model_name, filename=\"config.json\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# Modify config.json and save it\n",
    "config_data[\"rope_scaling\"] = {\"factor\": 8.0, \"type\": \"dynamic\"}\n",
    "with open(\"fixed_config.json\", 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "    \n",
    "# Load the modified config.json\n",
    "config = PretrainedConfig.from_json_file(\"fixed_config.json\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Filtering and processing data by cluster\n",
    "def generate_cluster_reviews(df, cluster_name):\n",
    "    cluster_df = df[df['cluster'] == cluster_name]\n",
    "    # Use only relevant fields for summarization\n",
    "    reviews = cluster_df[['review', 'rating', 'sentiment', 'sentiment_score', 'name']]\n",
    "    return reviews \"\"\"\n",
    "\n",
    "# Filtering and processing data by cluster [ONLY 10 SAMPLES]\n",
    "def generate_cluster_reviews(df, cluster_name):\n",
    "    cluster_df = df[df['cluster'] == cluster_name].sample(n=10, random_state=42)\n",
    "    # Use only relevant fields for summarization\n",
    "    reviews = cluster_df[['review', 'rating', 'sentiment', 'sentiment_score', 'name']]\n",
    "    return reviews\n",
    "\n",
    "# Format the input prompt for summarization\n",
    "def format_prompt(reviews):\n",
    "    prompt = \"\"\"\n",
    "        The following are customer reviews of various products in the category {category_name}. Based on these reviews, generate a detailed summary that provides:\n",
    "        - The Top 3 products and key differences between them. When should a consumer choose one or another?\n",
    "        - Top complaints for each of those products\n",
    "        - The worst product in the category and why customers should avoid it.\n",
    "        Reviews:\n",
    "    \"\"\"\n",
    "    prompt += \"\\n\".join(f\"Product Name: {row['name']}, Review: {row['review']}, Rating: {row['rating']}, Sentiment: {row['sentiment']}, Score: {row['sentiment_score']}\" for _, row in reviews.iterrows())\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters:   0%|          | 0/4 [00:00<?, ?cluster/s]\n",
      "Generating summaries: 0chunk [00:00, ?chunk/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate article using the Llama model\n",
    "def chunk_text(text, max_length):\n",
    "    \"\"\"Splits text into chunks of max_length tokens\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        yield tokenizer.convert_tokens_to_string(tokens[i:i+max_length])\n",
    "\n",
    "def generate_summary_from_chunks(model, tokenizer, prompt, max_length=2048):\n",
    "    summaries = []\n",
    "    for chunk in tqdm(chunk_text(prompt, max_length), desc=\"Generating summaries\", unit=\"chunk\"):\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=512, num_beams=5, early_stopping=True)\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "# Sample data for quicker processing\n",
    "sampled_df = final_df.sample(frac=0.1, random_state=42)  # Sample 10% of the data\n",
    "\n",
    "# Iterate through clusters and generate summaries\n",
    "cluster_names = sampled_df['cluster'].unique()\n",
    "summary_results = {}\n",
    "for cluster_name in tqdm(cluster_names, desc=\"Processing clusters\", unit=\"cluster\"):\n",
    "    cluster_reviews = generate_cluster_reviews(sampled_df, cluster_name)\n",
    "    prompt = format_prompt(cluster_reviews)\n",
    "    summary = generate_summary_from_chunks(model, tokenizer, prompt)\n",
    "    summary_results[cluster_name] = summary\n",
    "    print(f\"Summary for {cluster_name}:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save summaries to a file\n",
    "with open('cluster_summaries.txt', 'w') as f:\n",
    "    for cluster, summary in summary_results.items():\n",
    "        f.write(f\"Summary for {cluster}:\\n{summary}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
