{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Pre-Processing\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Datasets and saving them locally\n",
    "This section is from older versions of the code. Once I had the originals I started using the next version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control variables for code execution\n",
    "DOWNLOAD_ORIGINAL_DATASETS = False\n",
    "SAVE_LOCAL_DATASETS = False\n",
    "\n",
    "if DOWNLOAD_ORIGINAL_DATASETS:\n",
    "    # Get all available configurations (sub-categories)\n",
    "    categories = [\n",
    "        \"raw_review_All_Beauty\",\n",
    "        \"raw_review_Electronics\",\n",
    "        \"raw_review_Office_Products\",\n",
    "        \"raw_meta_All_Beauty\",\n",
    "        \"raw_meta_Electronics\",\n",
    "        \"raw_meta_Office_Products\",\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty list to hold datasets\n",
    "    datasets_list = []\n",
    "\n",
    "    # Loop over each configuration and download the dataset\n",
    "    for category in categories:\n",
    "        dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", category)\n",
    "        datasets_list.append(dataset)\n",
    "\n",
    "if SAVE_LOCAL_DATASETS:\n",
    "    # Save each dataset locally\n",
    "    for ds, category in zip(datasets_list, categories):\n",
    "        ds.save_to_disk(f\"./{category}_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all individual Dataframes from local CSVs\n",
    "Executed in parallel to reduce load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel loading of pre-saved dataframes\n",
    "file_paths = [\n",
    "    \"./dataframes/small_df.csv\",\n",
    "    \"./dataframes/beauty_df.csv\",\n",
    "    \"./dataframes/electronics_df.csv\",\n",
    "    \"./dataframes/office_df.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_csv(path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a csv file into a polars dataframe\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the csv file\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame\n",
    "    \"\"\"\n",
    "    return pl.read_csv(path)\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dataframes = list(executor.map(load_csv, file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "### Defining Functions for Removing short Reviews, Cleaning nulls and Duplicates ans Slicing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the text of the review is too short\n",
    "def remove_short_reviews(dataframe: pl.DataFrame, min_word_count: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows from a polars dataframe where the text of the 'text' column\n",
    "    is shorter than 'min_word_count'\n",
    "\n",
    "    Args:\n",
    "        dataframe (pl.DataFrame)\n",
    "        min_word_count (int)\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame\n",
    "    \"\"\"\n",
    "    return dataframe.filter(pl.col(\"text\").str.split(\" \").list.len() >= min_word_count)\n",
    "\n",
    "\n",
    "# Remove Duplicates and Nulls.\n",
    "# Lazy is used for more efficient execution of the multiple operations in the dataframes\n",
    "def clean_data(dataframe: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes Duplicates and None values from a pandas dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe (pl.DataFrame)\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame\n",
    "    \"\"\"\n",
    "    df_cleaned = dataframe.lazy().unique(subset=[\"text\"]).drop_nulls()\n",
    "    return df_cleaned.collect()\n",
    "\n",
    "\n",
    "# Split into n parts using slicing\n",
    "# Used in previous version sfor processing large datasets\n",
    "def slice_pl_df(dataframe: pl.DataFrame, slices: int) -> list[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Slices a polars dataframes into 'slices' parts\n",
    "\n",
    "    Args:\n",
    "        dataframe (pl.DataFrame)\n",
    "        slices (int): Number of slices or parts\n",
    "\n",
    "    Returns:\n",
    "        list[pl.DataFrame]: \n",
    "    \"\"\"\n",
    "    slice_size = dataframe.shape[0] // slices\n",
    "    dfs = []\n",
    "    for df_slice in tqdm(range(slices)):\n",
    "        start_idx = df_slice * slice_size\n",
    "        if df_slice == slices - 1:  # Ensure the last slice takes all remaining rows\n",
    "            dfs.append(dataframe[start_idx:])\n",
    "        else:\n",
    "            end_idx = (df_slice + 1) * slice_size\n",
    "            dfs.append(dataframe[start_idx:end_idx])\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing each Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9986.44it/s], ?it/s]\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "Slicing dataframes: 100%|██████████| 4/4 [00:00<00:00, 251.85it/s]\n",
      "Cleaning dataframes: 100%|██████████| 4/4 [00:29<00:00,  7.46s/it]\n",
      "Converting back to pandas: 100%|██████████| 4/4 [00:48<00:00, 12.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Drop unwanted columns and null values\n",
    "selected_columns = [\n",
    "    \"title\",\n",
    "    \"text\",\n",
    "    \"rating\",\n",
    "    \"id\",\n",
    "    \"parent_asin\",\n",
    "    \"name\",\n",
    "    \"categories\",\n",
    "]\n",
    "dataframes = [df[selected_columns] for df in dataframes]\n",
    "\n",
    "# Slice large dataframes into smaller chunks to improve memory efficiency\n",
    "for i, df in enumerate(tqdm(dataframes, desc=\"Slicing dataframes\")):\n",
    "    if df.height > 1_000_000:\n",
    "        dataframes[i] = slice_pl_df(df, 10)\n",
    "\n",
    "# Clean and preprocess the dataframes.\n",
    "# If a dataframe was sliced in the previous step, clean and preprocess each slice separately\n",
    "for i, df in enumerate(tqdm(dataframes, desc=\"Cleaning dataframes\")):\n",
    "    if isinstance(df, list):\n",
    "        cleaned_dfs = []\n",
    "        for sub_df in df:\n",
    "            sub_df = clean_data(sub_df)\n",
    "            sub_df = remove_short_reviews(sub_df, 25)\n",
    "            cleaned_dfs.append(sub_df)\n",
    "        dataframes[i] = pl.concat(cleaned_dfs)\n",
    "    else:\n",
    "        df = clean_data(df)\n",
    "        df = remove_short_reviews(df, 25)\n",
    "        dataframes[i] = df\n",
    "\n",
    "# Convert back to pandas for further processing.\n",
    "# I didn't have the time to adapt the next cell to use polars or my slicing function properly.\n",
    "dataframes = [\n",
    "    df.to_pandas() for df in tqdm(dataframes, desc=\"Converting back to pandas\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the final dataset\n",
    "The individual datasets are combined into one by sampling with an equal distribution of review ratings and individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling large dataframes: 100%|██████████| 4/4 [00:00<00:00,  4.03it/s]\n",
      "Sampling groups: 100%|██████████| 3/3 [00:01<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame shape: (97115, 8)\n",
      "\n",
      "Group distribution:\n",
      "group\n",
      "Positive    36357\n",
      "Neutral     30464\n",
      "Negative    30294\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "FINAL_DF_ROWS = 120_000\n",
    "LARGE_SAMPLE_SIZE = 1_000_000\n",
    "groups = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "rating_to_group = {\n",
    "    1: \"Negative\",\n",
    "    2: \"Negative\",\n",
    "    3: \"Neutral\",\n",
    "    4: \"Positive\",\n",
    "    5: \"Positive\",\n",
    "}\n",
    "\n",
    "# Replace the large datasets with sampled subsets\n",
    "small_dataframes = [\n",
    "    (\n",
    "        df.sample(n=LARGE_SAMPLE_SIZE, random_state=42)\n",
    "        if df.shape[0] > LARGE_SAMPLE_SIZE\n",
    "        else df\n",
    "    )\n",
    "    for df in tqdm(dataframes, desc=\"Sampling large dataframes\")\n",
    "]\n",
    "\n",
    "# Assign each rating to a group\n",
    "small_dataframes = [\n",
    "    df.assign(group=df[\"rating\"].map(rating_to_group)) for df in small_dataframes\n",
    "]\n",
    "\n",
    "# Determine total required rows per group\n",
    "all_group_counts = pd.concat([df[\"group\"] for df in small_dataframes]).value_counts()\n",
    "rows_per_group = FINAL_DF_ROWS // len(all_group_counts)\n",
    "\n",
    "# Initialize a list to collect samples\n",
    "final_samples = []\n",
    "\n",
    "# Allocate and sample data for each group\n",
    "for group in tqdm(groups, desc=\"Sampling groups\"):\n",
    "    # Calculate the total available rows for the group across all datasets\n",
    "    total_available = sum(df[df[\"group\"] == group].shape[0] for df in small_dataframes)\n",
    "    required_rows_adjusted = min(rows_per_group, total_available)\n",
    "\n",
    "    # Ideally, each dataset contributes equally\n",
    "    per_dataset_required = required_rows_adjusted // len(small_dataframes)\n",
    "\n",
    "    # Sample data from each dataset for the group\n",
    "    for df in small_dataframes:\n",
    "        df_group = df[df[\"group\"] == group]\n",
    "        available_rows = df_group.shape[0]\n",
    "        sample_size = min(per_dataset_required, available_rows)\n",
    "        if sample_size > 0:\n",
    "            sample_df = df_group.sample(n=sample_size, random_state=42)\n",
    "            final_samples.append(sample_df)\n",
    "\n",
    "# Concatenate all sampled data into the final DataFrame and drop the 'group' column\n",
    "final_df = pd.concat(final_samples, ignore_index=True)\n",
    "final_df = final_df.drop(columns=[\"group\"]).reset_index(drop=True)\n",
    "\n",
    "# Verify the final DataFrame\n",
    "print(f\"\\nFinal DataFrame shape: {final_df.shape}\")\n",
    "print(\"\\nGroup distribution:\")\n",
    "print(final_df[\"group\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV or Pickle atomically\n",
    "FORMAT = \"csv\"\n",
    "\n",
    "if FORMAT == \"csv\":\n",
    "    final_df.to_csv(\"final_dataset.csv.tmp\", index=False)\n",
    "    os.replace(\"final_dataset.csv.tmp\", \"final_dataset.csv\")\n",
    "else:\n",
    "    final_df.to_pickle(\"final_dataset.pkl.tmp\")\n",
    "    os.replace(\"final_dataset.pkl.tmp\", \"final_dataset.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoboReviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
